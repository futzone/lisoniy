# 10-Mavzu: ONNX yordamida Ishlab Chiqarish Uchun Modellarni Optimallashtirish

## Prototipdan Ishlab Chiqarishgacha Bo'lgan Ko'prik

Siz ajoyib STT modelini o'qitdingiz yoki fine-tune qildingiz. U sizning Jupyter Notebook-ingizda ajoyib ishlaydi. Lekin uni qanday qilib real dasturda, masalan, veb-saytda yoki mobil ilovada tez va samarali ishlatish mumkin?

Bu yerda **ONNX** yordamga keladi.

---

### ONNX Nima?

**ONNX (Open Neural Network Exchange)** - bu neyron tarmoq modellari uchun ochiq, standartlashtirilgan format. Uni turli deep learning freymvorklari (PyTorch, TensorFlow, Keras) o'rtasida "universal tarjimon" deb tasavvur qiling.

Asosiy g'oya shundaki, siz modelingizni bir freymvorkda (masalan, PyTorch) yaratib, uni ONNX formatiga aylantirasiz. Keyin, ushbu ONNX faylini turli xil platformalar va qurilmalarda ishga tushirishingiz mumkin, buning uchun asl freymvork kerak bo'lmaydi.

---

### Nima Uchun ONNX Ishlab Chiqarish Uchun Juda Muhim?

1.  **Platformadan Mustaqillik (Interoperability):**
    -   PyTorch-da o'qitilgan modelni TensorFlow Serving-da ishga tushiring.
    -   Modelni veb-brauzerda (`ONNX.js`), mobil qurilmalarda (iOS/Android) yoki hatto maxsus edge qurilmalarida (NVIDIA Jetson, Raspberry Pi) deploy qiling.

2.  **Tezroq Inference (Faster Inference):**
    -   **ONNX Runtime** - bu ONNX modellarini ishga tushirish uchun maxsus optimallashtirilgan yuqori samarali dvijok.
    -   U apparat ta'minotining o'ziga xos xususiyatlaridan (masalan, Intel MKL, NVIDIA TensorRT) foydalanib, standart PyTorch yoki TensorFlow-ga qaraganda **2x dan 10x gacha tezroq** ishlashi mumkin.

3.  **Model Hajmini Kichraytirish (Quantization):**
    -   ONNX modellarini kvantizatsiya qilish oson. Bu modeldagi 32-bitli (FP32) sonlarni 16-bitli (FP16) yoki 8-bitli (INT8) sonlarga o'tkazish jarayonidir.
    -   **Natija:**
        -   Model hajmi **2x-4x gacha kichrayadi**.
        -   Inference tezligi **1.5x-3x gacha oshadi**.
        -   Bu, ayniqsa, mobil va edge qurilmalari uchun juda muhim.

4.  **Python-ga Bog'liqlikni Yo'qotish:**
    -   ONNX Runtime C++, C#, Java, va JavaScript kabi tillar uchun API-larni taqdim etadi. Bu sizning Python-da o'qitilgan modelingizni boshqa tillarda yozilgan ilovalarga osonlikcha integratsiya qilish imkonini beradi.

---

### Whisper Modelini ONNX-ga Aylantirish (Misol)

Keling, HuggingFace `transformers` kutubxonasi va `optimum` yordamida Whisper modelini ONNX formatiga qanday o'tkazishni ko'rib chiqaylik.

**1. Kerakli kutubxonalarni o'rnating:**
```bash
pip install optimum[onnxruntime-gpu] # GPU uchun
# yoki
pip install optimum[onnxruntime]    # CPU uchun
```

**2. Aylantirish uchun Python skripti:**
```python
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq

# Modelni HuggingFace Hub dan yuklash va ONNX ga eksport qilish
# 'export=True' parametri hamma ishni avtomatik bajaradi
model = ORTModelForSpeechSeq2Seq.from_pretrained(
    "openai/whisper-small",  # Whisper modelini tanlang
    export=True
)

# Optimallashtirilgan modelni saqlash
model.save_pretrained("./whisper-small-onnx")

print("Whisper modeli muvaffaqiyatli ONNX formatiga o'girildi va saqlandi!")
```

Bu skript ishga tushirilgandan so'ng, `whisper-small-onnx` papkasida `encoder_model.onnx`, `decoder_model.onnx` va `decoder_with_past_model.onnx` kabi fayllar paydo bo'ladi. Endi siz ushbu fayllarni ONNX Runtime yordamida har qanday qo'llab-quvvatlanadigan platformada ishlatishingiz mumkin.

**3. ONNX Runtime bilan Inference:**
```python
from transformers import WhisperProcessor
from optimum.onnxruntime import ORTModelForSpeechSeq2Seq
import librosa

# Processor va ONNX modelini yuklash
processor = WhisperProcessor.from_pretrained("openai/whisper-small")
model = ORTModelForSpeechSeq2Seq.from_pretrained("./whisper-small-onnx/")

# Audioni qayta ishlash
audio, rate = librosa.load("audio.mp3", sr=16000)
input_features = processor(audio, sampling_rate=rate, return_tensors="pt").input_features

# Matn yaratish (bu endi ONNX Runtime orqali ishlaydi)
predicted_ids = model.generate(input_features)

# Natijani dekodlash
transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

print(transcription)
```

Bu yondashuv sizga Python-ning qulayligini saqlagan holda ONNX Runtime-ning tezligidan foydalanish imkonini beradi.

**Xulosa:** ONNX - bu STT (va boshqa) modellaringizni laboratoriyadan olib, real dunyo ilovalariga joylashtirish uchun muhim va zaruriy qadamdir. U tezlik, samaradorlik va platformalararo moslashuvchanlikni ta'minlaydi.